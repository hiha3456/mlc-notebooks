{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLC 作业 1: 端到端模型执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分: 模型准备\n",
    "\n",
    "本作业的目标是让你对机器学习编译过程中的端到端模型的执行和变换更加熟悉。让我们从一个简单的图像分类模型开始。\n",
    "\n",
    "我们首先使用如下的命令来安装必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
    "# !python3 -m pip install torch torchvision torchaudio torchsummary --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import tvm\n",
    "import tvm.testing\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from tvm import topi, relax, te\n",
    "from tvm.script import tir as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是用PyTorch定义的模型。该模型接受一批图像为输入，然后对它们依次作用卷积层，激活层，池化层和全连接层，得到分类结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_shape = (batch_size, 1, 28, 28)  # NCHW layout\n",
    "\n",
    "\n",
    "def pytorch_model():\n",
    "    list = []\n",
    "    list.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), bias=True))\n",
    "    list.append(nn.ReLU())\n",
    "    list.append(nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "    list.append(nn.Flatten())\n",
    "    list.append(nn.Linear(in_features=5408, out_features=100, bias=True))\n",
    "    list.append(nn.ReLU())\n",
    "    list.append(nn.Linear(in_features=100, out_features=10, bias=True))\n",
    "    list.append(nn.Softmax(dim=1))\n",
    "\n",
    "    model = nn.Sequential(*list).cpu()\n",
    "    name_map = {\n",
    "        \"0.weight\": \"conv2d_weight\",\n",
    "        \"0.bias\": \"conv2d_bias\",\n",
    "        \"4.weight\": \"linear0_weight\",\n",
    "        \"4.bias\": \"linear0_bias\",\n",
    "        \"6.weight\": \"linear1_weight\",\n",
    "        \"6.bias\": \"linear1_bias\",\n",
    "    }\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = torch.from_numpy(weight_map[name_map[name]]).cpu()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提供了一个在Fashion MNIST数据集上的预训练权重图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide outputs\n",
    "# !wget -nc https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_assignment_params.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到它的准确率约为84%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: Ankle boot, label: Ankle boot\n",
      "\n",
      "Test set: Average loss: -0.8369, Accuracy: 8388/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the weight map from file.\n",
    "# The prediction accuracy of the weight map on test data is around 83.3%.\n",
    "weight_map = pkl.load(open(\"fasionmnist_mlp_assignment_params.pkl\", \"rb\"))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        print_img = True\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.cpu(), label.cpu()\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, label, reduction=\"sum\").item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            if print_img:\n",
    "                imshow(data[0])\n",
    "                print(\"predict: {}, label: {}\".format(class_names[pred[0][0]], class_names[label[0]]))\n",
    "                print_img = False\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=False)\n",
    "test(pytorch_model(), test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "              ReLU-2           [-1, 32, 26, 26]               0\n",
      "         MaxPool2d-3           [-1, 32, 13, 13]               0\n",
      "           Flatten-4                 [-1, 5408]               0\n",
      "            Linear-5                  [-1, 100]         540,900\n",
      "              ReLU-6                  [-1, 100]               0\n",
      "            Linear-7                   [-1, 10]           1,010\n",
      "           Softmax-8                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 542,230\n",
      "Trainable params: 542,230\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.41\n",
      "Params size (MB): 2.07\n",
      "Estimated Total Size (MB): 2.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch_model = pytorch_model()\n",
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "t = torch_model.to(device)\n",
    "summary(torch_model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分: 从PyTorch迁移模型\n",
    "为了展示机器学习编译对端到端模型的抽象，我们需要将模型从PyTorch迁移并转换为TVMScript实现。然后，手工迁移很难。正如你在TensorIR练习中所体验的那样，为模型中的每一层写一个元张量函数需要大量的人力来完成。另外，手工写这些函数是容易犯错的。你可以想象，当你写了几百行，但其中有零星几个bug，那么找到bug的过程将会是痛苦的。\n",
    "\n",
    "幸运的是，在TVM中有一个简单的多的方法能够迁移模型。TVM提供了一个类`relax.BlockBuilder`，它能够从空白的IRModule开始一步步的构建端到端模型。（回忆我们在第四节课中介绍的Relax的Dataflow Block，这里的\"block\"就是代表了Relax函数中的Dataflow Block）\n",
    "\n",
    "具体而言，在 `BlockBuilder`中我们有一个 `emit_te`的API，它可以将一个张量表达式（第三节课中介绍过）的算子描述转变成一个对应TensorIR函数的`call_tir`操作（`call_tir`在第四节课中介绍过）。与手工写TensorIR函数相比，写张量表达式描述可以用几行代码来完成，这减少了需要的工作量和犯错的概率。\n",
    "\n",
    "`emit_te`的函数签名是`emit_te(func, *input)`，其中`func`是一个返回张量表达式的函数，而`*input`是`func`的输入。\n",
    "\n",
    "让我们从一个例子开始详细介绍。在下方的代码块中，`relu`是一个返回ReLU算子的张量表达式描述的函数。为了构建一个执行单个ReLU算子的Relax函数，在`emit_te_example`中我们首先定义了一个`BlockBuilder`实例`bb`。我们也定义了一个2维128x128大小的张量变量`x`，它将作为ReLU操作的输入张量（同时也是Relax函数的输入）。\n",
    "\n",
    "在这之后，我们用`with bb.function(name, [*input])` API构建一个以`x`为输入的Relax函数 `main`。然后我们构建一个dataflow block。在这个dataflow block里，我们首先用`emit_te`生成一个调用ReLU算子的`call_tir`。这里 `emit_te`在IRModule中生成了一个名字为`relu`的TensorIR函数，然后在dataflow block中生成`call_tir(relu, (x,), (128, 128), dtype=\"float32\")`操作。`call_tir`之后是函数返回。\n",
    "\n",
    "在这一构造之后，BlockBuilder实例`bb`包含构建完的IRModule，它可以通过`bb.get()`得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(A):\n",
    "    B = te.compute(shape=(128, 128), fcompute=lambda i, j: te.max(A[i, j], 0), name=\"B\")\n",
    "    return B\n",
    "\n",
    "\n",
    "def emit_te_example():\n",
    "    bb = relax.BlockBuilder()\n",
    "    # x = relax.Var(\"x\", (128, 128), relax.DynTensorType(2, \"float32\"))\n",
    "    x = relax.Var(\"x\", relax.TensorStructInfo((128, 128), \"float32\"))\n",
    "    with bb.function(\"main\", [x]):\n",
    "        with bb.dataflow():\n",
    "            lv0 = bb.emit_te(relu, x)\n",
    "            gv = bb.emit_output(lv0)\n",
    "        bb.emit_func_output(gv)\n",
    "    return bb.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数`emit_te_example`返回构造得到的IRModule。为了看的更清楚，我们可以输出这一IRModule。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\tvm-build-clone\\lib\\site-packages\\tvm\\script\\highlight.py:117: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "d:\\miniconda3\\envs\\tvm-build-clone\\python.exe -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">relu</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_i, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_i, v_j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_i, v_j])\n",
       "                B[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(A[v_i, v_j], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu, (x,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "mod = emit_te_example()\n",
    "mod.show()\n",
    "# IPython.display.Code(mod.script(), language=\"python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你看到的，通过BlockBuilder生成的IRModule确实包含了ReLU的TensorIR实现和一个含有调用ReLU实现的`call_tir`的Relax函数\n",
    "\n",
    "现在轮到你来用BlockBuilder和`emit_te`来创建一个和之前定义的PyTorch模型等价的IRModule。你可以自己为所有的算子写张量表达式描述。或者，TVM提供了TOPI（TVM Operator Inventory）库，它为不同的算子提供了张量表达式描述。如果你愿意阅读[文档](https://tvm.apache.org/docs/reference/api/python/topi.html)来弄懂它的用法，这也是被鼓励的。我们提供了测试函数来检查你的IRModule的正确性。\n",
    "\n",
    "注意到每个Conv2d层和linear层都包含了一个偏置加法，这应该在你构建的IRModule中被体现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d(A, W, B):\n",
    "    cin = te.reduce_axis((0, 1), \"cin\")\n",
    "    di = te.reduce_axis((0, 3), \"di\")\n",
    "    dj = te.reduce_axis((0, 3), \"dj\")\n",
    "    \n",
    "    # a bug here, reduction must be at top level of compute\n",
    "    # C = te.compute(shape=(batch_size, 32, 26, 26), fcompute=lambda n, cout, i, j: \\\n",
    "    #                B[0, cout, 0, 0] + te.sum(A[n, cin, 1*i + di, 1*j + dj]*W[cout, cin, di, dj], axis=(cin, di, dj)), name=\"C\")\n",
    "\n",
    "\n",
    "    C = te.compute(shape=(batch_size, 32, 26, 26), fcompute=lambda n, cout, i, j: \\\n",
    "                   te.sum(A[n, cin, 1*i + di, 1*j + dj]*W[cout, cin, di, dj], axis=(cin, di, dj)), name=\"C\")\n",
    "    Res = te.compute(shape=(batch_size, 32, 26, 26), fcompute=lambda n, cout, i, j: \\\n",
    "                   C[n, cout, i, j] + B[0, cout, 0, 0], name=\"Res\")\n",
    "    return Res\n",
    "\n",
    "def Relu1(A):\n",
    "    B = te.compute(shape=(batch_size, 32, 26, 26), fcompute=lambda n, c, i, j: te.max(A[n, c, i, j], 0), name=\"B\")\n",
    "    return B\n",
    "\n",
    "def MaxPool2d(A):\n",
    "    di = te.reduce_axis((0, 2), \"di\")\n",
    "    dj = te.reduce_axis((0, 2), \"dj\")\n",
    "    B = te.compute(shape=(batch_size, 32, 13, 13), fcompute=lambda n, c, i, j: \\\n",
    "                   te.max(A[n, c, 2*i+di, 2*j+dj], axis=(di, dj)), name=\"B\"\n",
    "                )\n",
    "    return B\n",
    "\n",
    "def Flatten(A):\n",
    "    B = te.compute(shape=(batch_size, 5408), fcompute=lambda n, i: A[n, i//(13*13), (i % (13*13))//13, (i % (13*13)) % 13], name=\"B\")\n",
    "    return B\n",
    "\n",
    "def Linear1(A, W, B):\n",
    "    j = te.reduce_axis((0, 5408), \"j\")\n",
    "    C = te.compute(shape=(batch_size, 100), fcompute=lambda n, i: te.sum(A[n, j] * W[i, j], axis=j), name=\"C\")\n",
    "    Res = te.compute(shape=(batch_size, 100), fcompute=lambda n, i: C[n, i] + B[0, i], name=\"Res\")\n",
    "    return Res\n",
    "\n",
    "def Relu2(A):\n",
    "    B = te.compute(shape=(batch_size, 100), fcompute=lambda n, i: te.max(A[n, i], 0), name=\"B\")\n",
    "    return B\n",
    "\n",
    "def Linear2(A, W, B):\n",
    "    j = te.reduce_axis((0, 100), \"j\")\n",
    "    C = te.compute(shape=(batch_size, 10), fcompute=lambda n, i: te.sum(A[n, j] * W[i, j], axis=j), name=\"C\")\n",
    "    Res = te.compute(shape=(batch_size, 10), fcompute=lambda n, i: C[n, i] + B[0, i], name=\"Res\")\n",
    "    return Res\n",
    "\n",
    "def Softmax(A):\n",
    "    i = te.reduce_axis((0, 10), \"i\")\n",
    "    sum_exp = te.compute(shape=(batch_size, 1), fcompute=lambda n: te.sum(te.exp(A[n, i]), axis=i), name='sum_exp')\n",
    "    B = te.compute(shape=(batch_size, 10), fcompute=lambda n, j: te.exp(A[n, j])/sum_exp[n], name=\"B\")\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\tvm-build-clone\\lib\\site-packages\\tvm\\script\\highlight.py:117: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "d:\\miniconda3\\envs\\tvm-build-clone\\python.exe -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Conv2d</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">28</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">28</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Res: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        C_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, cout, i, j, cin, di, dj <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">3</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                v_n, v_cout, v_i, v_j, v_cin, v_di, v_dj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, cout, i, j, cin, di, dj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_cin, v_i <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_dj], B[v_cout, v_cin, v_di, v_dj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C_1[v_n, v_cout, v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    C_1[v_n, v_cout, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                C_1[v_n, v_cout, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_cout, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_n, v_cin, v_i <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_dj] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_cout, v_cin, v_di, v_dj]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, cout, i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Res&quot;</span>):\n",
       "                v_n, v_cout, v_i, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, cout, i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(C_1[v_n, v_cout, v_i, v_j], C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_cout, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Res[v_n, v_cout, v_i, v_j])\n",
       "                Res[v_n, v_cout, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_cout, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_cout, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>)]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Flatten</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5408</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5408</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">169</span>), v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">169</span>) <span style=\"color: #AA22FF; font-weight: bold\">//</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>)])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_n, v_i])\n",
       "                B[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">169</span>), v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">169</span>) <span style=\"color: #AA22FF; font-weight: bold\">//</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>)]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Linear1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5408</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5408</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Res: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        C_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">5408</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                v_n, v_i, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_j], B[v_i, v_j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C_1[v_n, v_i])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_n, v_j] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_i, v_j]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Res&quot;</span>):\n",
       "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(C_1[v_n, v_i], C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Res[v_n, v_i])\n",
       "                Res[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">+</span> C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_i]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Linear2</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Res: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        C_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                v_n, v_i, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_j], B[v_i, v_j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C_1[v_n, v_i])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_n, v_j] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_i, v_j]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Res&quot;</span>):\n",
       "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(C_1[v_n, v_i], C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Res[v_n, v_i])\n",
       "                Res[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C_1[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">+</span> C[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">0</span>), v_i]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">MaxPool2d</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, i, j, di, dj <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">13</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_n, v_c, v_i, v_j, v_di, v_dj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, i, j, di, dj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_c, v_i <span style=\"color: #AA22FF; font-weight: bold\">*</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #AA22FF; font-weight: bold\">*</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_dj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_n, v_c, v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    B[v_n, v_c, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">3.4028234663852886e+38</span>)\n",
       "                B[v_n, v_c, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(B[v_n, v_c, v_i, v_j], A[v_n, v_c, v_i <span style=\"color: #AA22FF; font-weight: bold\">*</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #AA22FF; font-weight: bold\">*</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">2</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_dj])\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Relu1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">32</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">26</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_n, v_c, v_i, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_c, v_i, v_j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_n, v_c, v_i, v_j])\n",
       "                B[v_n, v_c, v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(A[v_n, v_c, v_i, v_j], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Relu2</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">100</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_n, v_i])\n",
       "                B[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(A[v_n, v_i], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">Softmax</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        sum_exp <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>),))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;sum_exp&quot;</span>):\n",
       "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SR&quot;</span>, [n, i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_i])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(sum_exp[v_n])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    sum_exp[v_n] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                sum_exp[v_n] <span style=\"color: #AA22FF; font-weight: bold\">=</span> sum_exp[v_n] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>exp(A[v_n, v_i])\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">4</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;B&quot;</span>):\n",
       "                v_n, v_j <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_n, v_j], sum_exp[v_n])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[v_n, v_j])\n",
       "                B[v_n, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>exp(A[v_n, v_j]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> sum_exp[v_n]\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">28</span>, <span style=\"color: #008000\">28</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Conv2d, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">26</span>, <span style=\"color: #008000\">26</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Relu1, (lv,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">26</span>, <span style=\"color: #008000\">26</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>MaxPool2d, (lv1,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">13</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Flatten, (lv2,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">5408</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Linear1, (lv3, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">100</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv5 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Relu2, (lv4,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">100</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv6 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Linear2, (lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">4</span>], metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">5</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv7 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>Softmax, (lv6,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv7\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model_via_emit_te():\n",
    "    bb = relax.BlockBuilder()\n",
    "    # x = relax.Var(\"x\", input_shape, relax.DynTensorType(batch_size, \"float32\"))\n",
    "    x = relax.Var(\"x\", relax.TensorStructInfo(input_shape, \"float32\"))\n",
    "\n",
    "    conv2d_weight = relax.const(weight_map[\"conv2d_weight\"], \"float32\")\n",
    "    conv2d_bias = relax.const(weight_map[\"conv2d_bias\"].reshape(1, 32, 1, 1), \"float32\")\n",
    "    linear0_weight = relax.const(weight_map[\"linear0_weight\"], \"float32\")\n",
    "    linear0_bias = relax.const(weight_map[\"linear0_bias\"].reshape(1, 100), \"float32\")\n",
    "    linear1_weight = relax.const(weight_map[\"linear1_weight\"], \"float32\")\n",
    "    linear1_bias = relax.const(weight_map[\"linear1_bias\"].reshape(1, 10), \"float32\")\n",
    "\n",
    "    with bb.function(\"main\", [x]):\n",
    "        with bb.dataflow():\n",
    "           lv0 = bb.emit_te(Conv2d, x, conv2d_weight, conv2d_bias)\n",
    "           lv1 = bb.emit_te(Relu1, lv0)\n",
    "           lv2 = bb.emit_te(MaxPool2d, lv1)\n",
    "           lv3 = bb.emit_te(Flatten, lv2)\n",
    "           lv4 = bb.emit_te(Linear1, lv3, linear0_weight, linear0_bias)\n",
    "           lv5 = bb.emit_te(Relu2, lv4)\n",
    "           lv6 = bb.emit_te(Linear2, lv5, linear1_weight, linear1_bias)\n",
    "           lv7 = bb.emit_te(Softmax, lv6)\n",
    "           gv = bb.emit_output(lv7)\n",
    "        bb.emit_func_output(gv)\n",
    "    return bb.get()\n",
    "\n",
    "\n",
    "\n",
    "def build_mod(mod):\n",
    "    exec = relax.build(mod, \"llvm\")\n",
    "    dev = tvm.cpu()\n",
    "    vm = relax.VirtualMachine(exec, dev)\n",
    "    return vm\n",
    "\n",
    "\n",
    "def check_equivalence(mod, torch_model, test_loader):\n",
    "    torch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        rt_mod = build_mod(mod)\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.cpu(), label.cpu()\n",
    "            output_from_pytorch = torch_model(data).numpy()\n",
    "            output_from_relax = rt_mod[\"main\"](tvm.nd.array(data, tvm.cpu())).numpy()\n",
    "            tvm.testing.assert_allclose(output_from_pytorch, output_from_relax, rtol=1e-4)\n",
    "\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "mod = create_model_via_emit_te()\n",
    "torch_model = pytorch_model()\n",
    "\n",
    "check_equivalence(mod, torch_model, test_loader)\n",
    "mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分: 使用库\n",
    "\n",
    "正如我们在第四节课中谈到的，我们可以将torch函数整合进IRModule。步骤包括注册一个外部运行时函数，和在IRModule中用`call_tir`调用。\n",
    "\n",
    "这里是一个用torch matmul和torch add拉力实现一个linear层的例子。你也可以在第四节课的笔记中找到这个例子。\n",
    "\n",
    "\n",
    "```python\n",
    "@tvm.register_func(\"env.linear\", override=True)\n",
    "def torch_linear(x: tvm.nd.NDArray,\n",
    "                 w: tvm.nd.NDArray,\n",
    "                 b: tvm.nd.NDArray,\n",
    "                 out: tvm.nd.NDArray):\n",
    "    x_torch = torch.from_dlpack(x)\n",
    "    w_torch = torch.from_dlpack(w)\n",
    "    b_torch = torch.from_dlpack(b)\n",
    "    out_torch = torch.from_dlpack(out)\n",
    "    torch.mm(x_torch, w_torch.T, out=out_torch)\n",
    "    torch.add(out_torch, b_torch, out=out_torch)\n",
    "\n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyModuleWithExternCall:\n",
    "    @R.function\n",
    "    def main(x: Tensor((1, 784), \"float32\"),\n",
    "             w0: Tensor((128, 784), \"float32\"),\n",
    "             b0: Tensor((128,), \"float32\")):\n",
    "        # block 0\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_tir(\"env.linear\", (x, w0, b0), (1, 128), dtype=\"float32\")\n",
    "            ...\n",
    "        return ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请为你在第二部分中创建的IRModule中的卷积层注册外部函数。你需要使用NumPy或者PyTorch作为你的函数实现。\n",
    "\n",
    "你可能需要使用`BlockBuilder.emit`在正在构建的Relax函数的结尾直接添加一个`call_tir`操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "struct_info needs to be an instance of StructInfo. If you attempt to pass in shape, use relax.TensorStructInfo(shape, dtype).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m bb\u001b[39m.\u001b[39mget()\n\u001b[0;32m     22\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test_data, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 23\u001b[0m mod \u001b[39m=\u001b[39m create_model_with_torch_func()\n\u001b[0;32m     24\u001b[0m check_equivalence(mod, torch_model, test_loader)\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36mcreate_model_with_torch_func\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model_with_torch_func\u001b[39m():\n\u001b[0;32m      2\u001b[0m     bb \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mBlockBuilder()\n\u001b[1;32m----> 4\u001b[0m     x \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39;49mVar(\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m, input_shape, relax\u001b[39m.\u001b[39;49mDynTensorType(\u001b[39m4\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m     conv2d_weight \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mconst(weight_map[\u001b[39m\"\u001b[39m\u001b[39mconv2d_weight\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     conv2d_bias \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mconst(weight_map[\u001b[39m\"\u001b[39m\u001b[39mconv2d_bias\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\tvm-build-clone\\lib\\site-packages\\tvm\\relax\\expr.py:408\u001b[0m, in \u001b[0;36mVar.__init__\u001b[1;34m(self, name_hint, struct_info, span)\u001b[0m\n\u001b[0;32m    406\u001b[0m     struct_info \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39mconvert_to_object(struct_info)\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(struct_info, StructInfo):\n\u001b[1;32m--> 408\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mstruct_info needs to be an instance of StructInfo. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf you attempt to pass in shape, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39muse relax.TensorStructInfo(shape, dtype).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         )\n\u001b[0;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__init_handle_by_constructor__(\n\u001b[0;32m    414\u001b[0m     _ffi_api\u001b[39m.\u001b[39mVar \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(name_hint, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _ffi_api\u001b[39m.\u001b[39mVarFromId,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     name_hint,\n\u001b[0;32m    416\u001b[0m     struct_info,\n\u001b[0;32m    417\u001b[0m     span,\n\u001b[0;32m    418\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: struct_info needs to be an instance of StructInfo. If you attempt to pass in shape, use relax.TensorStructInfo(shape, dtype)."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_model_with_torch_func():\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    x = relax.Var(\"x\", input_shape, relax.DynTensorType(4, \"float32\"))\n",
    "\n",
    "    conv2d_weight = relax.const(weight_map[\"conv2d_weight\"], \"float32\")\n",
    "    conv2d_bias = relax.const(weight_map[\"conv2d_bias\"].reshape(1, 32, 1, 1), \"float32\")\n",
    "    linear0_weight = relax.const(weight_map[\"linear0_weight\"], \"float32\")\n",
    "    linear0_bias = relax.const(weight_map[\"linear0_bias\"].reshape(1, 100), \"float32\")\n",
    "    linear1_weight = relax.const(weight_map[\"linear1_weight\"], \"float32\")\n",
    "    linear1_bias = relax.const(weight_map[\"linear1_bias\"].reshape(1, 10), \"float32\")\n",
    "\n",
    "    with bb.function(\"main\", [x]):\n",
    "        with bb.dataflow():\n",
    "            # TODO:\n",
    "            ...\n",
    "        bb.emit_func_output(gv)\n",
    "\n",
    "    return bb.get()\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "mod = create_model_with_torch_func()\n",
    "check_equivalence(mod, torch_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分: 端到端模型中的程序变换\n",
    "\n",
    "在TensorIR练习中, 我们学会了如何变换单个TensorIR函数。在端到端模型中变换是类似的。\n",
    "\n",
    "和批量矩阵乘法相比，让我们关注一个更加有挑战性的算子：conv2d（二维卷积）。\n",
    "\n",
    "首先，让我们介绍一些新的原语：\n",
    " - `compute_inline`：它将一个block内联到另一个block中，以减少内存使用大小和内存访问次数\n",
    " - `fuse`：和`split`相对。融合多个轴。这里`fuse`与`parallel` / `vectorize` / `unroll`一起使用，以增加并行度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.prim_func\n",
    "def before_inline(a: T.handle, c: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (128, 128))\n",
    "    B = T.alloc_buffer((128, 128))\n",
    "    C = T.match_buffer(c, (128, 128))\n",
    "    for i, j in T.grid(128, 128):\n",
    "        with T.block(\"B\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            B[vi, vj] = A[vi, vj] * 2.0\n",
    "    for i, j in T.grid(128, 128):\n",
    "        with T.block(\"C\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            C[vi, vj] = B[vi, vj] + 1.0\n",
    "\n",
    "\n",
    "sch = tvm.tir.Schedule(before_inline)\n",
    "sch.compute_inline(sch.get_block(\"B\"))\n",
    "IPython.display.Code(sch.mod[\"main\"].script(), language=\"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.prim_func\n",
    "def before_fuse(a: T.handle, b: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (128, 128))\n",
    "    B = T.match_buffer(b, (128, 128))\n",
    "    for i, j in T.grid(128, 128):\n",
    "        with T.block(\"B\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            B[vi, vj] = A[vi, vj] * 2.0\n",
    "\n",
    "\n",
    "sch = tvm.tir.Schedule(before_fuse)\n",
    "i, j = sch.get_loops(sch.get_block(\"B\"))\n",
    "sch.fuse(i, j)\n",
    "IPython.display.Code(sch.mod[\"main\"].script(), language=\"python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们首先为第二部分中得到的IRModule创建一个schedule，然后对其中的conv2d TensorIR函数变换。和TensorIR练习类似，我们提供了一个目标函数。但请注意，目标函数不是标准答案，原因如下：\n",
    " - 它可能不能在所有硬件中都取得最佳性能\n",
    " - 原始的conv2d TensorIR实现可能不同，这决定与你在第二部分中使用的张量表达式描述：\n",
    "   - 如果你将conv2d的计算和偏置加法的计算放在了一个张量表达式中，那么在变换完成的TensorIR函数的末尾应该有一个计算偏置加法的block\n",
    "   - 如果你将上述两个计算分开在不同的张量表达式，或者你使用了TOPI提供的conv2d，那么变换完成的TensorIR函数末尾不应该有计算偏置加法的block。下面给出的目标函数是用TOPI conv2d获得的TensorIR函数做变换后生成的。\n",
    "\n",
    "```python\n",
    "@T.prim_func\n",
    "def target_func(rxplaceholder: T.Buffer[(4, 1, 28, 28), \"float32\"], rxplaceholder_1: T.Buffer[(32, 1, 3, 3), \"float32\"], conv2d_nchw: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
    "    T.func_attr({\"global_symbol\": \"conv2d\", \"tir.noalias\": True})\n",
    "    # body\n",
    "    # with T.block(\"root\")\n",
    "    for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(2704):\n",
    "        for i0_1_i1_1_fused_init in T.unroll(8):\n",
    "            for i2_1_i3_1_fused_init in T.vectorized(4):\n",
    "                with T.block(\"conv2d_nchw_init\"):\n",
    "                    nn = T.axis.spatial(\n",
    "                        4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused_init // 4)\n",
    "                    ff = T.axis.spatial(\n",
    "                        32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused_init % 4)\n",
    "                    yy = T.axis.spatial(\n",
    "                        26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused_init // 2)\n",
    "                    xx = T.axis.spatial(\n",
    "                        26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused_init % 2)\n",
    "                    T.reads()\n",
    "                    T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
    "                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)\n",
    "        for i4, i5, i6 in T.grid(1, 3, 3):\n",
    "            for i0_1_i1_1_fused in T.unroll(8):\n",
    "                for i2_1_i3_1_fused in T.vectorized(4):\n",
    "                    with T.block(\"conv2d_nchw_update\"):\n",
    "                        nn = T.axis.spatial(\n",
    "                            4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused // 4)\n",
    "                        ff = T.axis.spatial(\n",
    "                            32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused % 4)\n",
    "                        yy = T.axis.spatial(\n",
    "                            26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused // 2)\n",
    "                        xx = T.axis.spatial(\n",
    "                            26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused % 2)\n",
    "                        rc, ry, rx = T.axis.remap(\"RRR\", [i4, i5, i6])\n",
    "                        T.reads(conv2d_nchw[nn, ff, yy, xx], rxplaceholder[nn,\n",
    "                                rc, yy + ry, xx + rx], rxplaceholder_1[ff, rc, ry, rx])\n",
    "                        T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
    "                        conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + \\\n",
    "                            rxplaceholder[nn, rc, yy + ry, xx +\n",
    "                                          rx] * rxplaceholder_1[ff, rc, ry, rx]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和TensorIR练习中不同的是, 这里schedule是为一个IRModule创建的，而不是TensorIR函数. 因此，当使用`sch.get_block`时，需要提供TensorIR函数名字，如下方所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = create_model_via_emit_te()\n",
    "sch = tvm.tir.Schedule(mod)\n",
    "\n",
    "# Step 1. Get blocks\n",
    "# block = sch.get_block(name=\"your_block_name\", func_name=\"your_function_name\")\n",
    "\n",
    "# Step 2. Inline the padding block (if exists)\n",
    "\n",
    "# Step 3. Get loops\n",
    "\n",
    "# Step 4. Organize the loops\n",
    "\n",
    "# Step 5. decompose reduction\n",
    "\n",
    "# Step 6. fuse + vectorize / fuse + parallel / fuse + unroll\n",
    "\n",
    "IPython.display.Code(sch.mod.script(), language=\"python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们可以测试变换后IRModule的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "check_equivalence(sch.mod, torch_model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
