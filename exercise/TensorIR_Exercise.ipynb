{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1. 第一节：如何编写 TensorIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1.1. 示例：逐位相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16, dtype=np.int64).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1, dtype=np.int64).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "    @T.prim_func\n",
    "    def add(A: T.Buffer((4,4), \"int64\"),\n",
    "            B: T.Buffer((4,4), \"int64\"),\n",
    "            C: T.Buffer((4,4), \"int64\")):\n",
    "        T.func_attr({\"global_symbol\": \"add\"})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(4, i)\n",
    "                vj = T.axis.spatial(4, j)\n",
    "                C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4,4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1.2. 练习 1：广播加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16, dtype=np.int64).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1, dtype=np.int64).reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def broadcast_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            c[i, j] = a[i, j] + b[j]\n",
    "\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "broadcast_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer((4,4), \"int64\"),\n",
    "          B: T.Buffer((4), \"int64\"),\n",
    "          C: T.Buffer((4,4), \"int64\")):\n",
    "    T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1.3. 练习 2：二维卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W, dtype=np.int64).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K, dtype=np.int64).reshape(CO, CI, K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int64)\n",
    "conv_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  @T.prim_func\n",
    "  def conv(data: T.Buffer((N, CI, H, W), \"int64\"),\n",
    "           weight: T.Buffer((CO, CI, K, K), \"int64\"),\n",
    "           conv: T.Buffer((N, CO, OUT_H, OUT_W), \"int64\")):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    for n, co, ho, wo, ci, di, dj in T.grid(N, CO, OUT_H, OUT_W, CI, K, K):\n",
    "      with T.block(\"CONV\"):\n",
    "        vn, vco, vho, vwo, vci, vdi, vdj = T.axis.remap(\"SSSSRRR\", [n, co, ho, wo, ci, di, dj])\n",
    "        with T.init():\n",
    "          conv[vn, vco, vho, vwo] = T.int64(0)\n",
    "        conv[vn, vco, vho, vwo] = conv[vn, vco, vho, vwo] + data[vn, vci, vho+vdi, vwo+vdj] * weight[vco, vci, vdi, vdj]\n",
    "\n",
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.2. 第二节：如何变换 TensorIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2.1. 并行化、向量化与循环展开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(\n",
       "        A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>),\n",
       "        B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>),\n",
       "        C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>),\n",
       "    ):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;add&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">2</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>unroll(<span style=\"color: #008000\">2</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">4</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                        vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">4</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
       "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">4</span>, j)\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vj], B[vi, vj])\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
       "                        C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vi, vj]\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer((4, 4), \"int64\"),\n",
    "          B: T.Buffer((4, 4), \"int64\"),\n",
    "          C: T.Buffer((4, 4), \"int64\")):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "sch = tvm.tir.Schedule(MyAdd)\n",
    "block = sch.get_block(\"C\", func_name=\"add\")\n",
    "i, j = sch.get_loops(block)\n",
    "i0, i1 = sch.split(i, factors=[2, 2])\n",
    "sch.parallel(i0)\n",
    "sch.unroll(i1)\n",
    "sch.vectorize(j)\n",
    "sch.mod.show()\n",
    "# IPython.display.Code(sch.mod.script(), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2.2. 练习 3：变换批量矩阵乘法程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">bmm_relu</span>(\n",
       "        A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;bmm_relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        Y <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y&quot;</span>):\n",
       "                vn, vi, vj, vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSR&quot;</span>, [n, i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vn, vi, vk], B[vn, vk, vj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vn, vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vn, vk, vj]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                vn, vi, vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSS&quot;</span>, [n, i, j])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vn, vi, vj])\n",
       "                C[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Y[vn, vi, vj], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyBmmRelu:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(A: T.buffer((16, 128, 128), \"float32\"),\n",
    "               B: T.buffer((16, 128, 128), \"float32\"),\n",
    "               C: T.buffer((16, 128, 128), \"float32\"),):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "    Y = T.alloc_buffer((16, 128, 128), dtype=\"float32\")\n",
    "    for n, i, j, k in T.grid(16, 128, 128, 128):\n",
    "      with T.block(\"Y\"):\n",
    "        vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
    "        with T.init():\n",
    "          Y[vn, vi, vj] = T.float32(0)\n",
    "        Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
    "    for n, i, j in T.grid(16, 128, 128):\n",
    "      with T.block(\"C\"):\n",
    "        vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
    "        C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "sch.mod.show()\n",
    "# IPython.display.Code(sch.mod.script(), language=\"python\")\n",
    "# Also please validate your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.83861612e+06, 1.83860925e+06, 1.83860212e+06, ...,\n",
       "         1.83770850e+06, 1.83770088e+06, 1.83769388e+06],\n",
       "        [5.58509200e+06, 5.58507100e+06, 5.58504900e+06, ...,\n",
       "         5.58235450e+06, 5.58233250e+06, 5.58231100e+06],\n",
       "        [9.33157000e+06, 9.33153200e+06, 9.33149400e+06, ...,\n",
       "         9.32700100e+06, 9.32696600e+06, 9.32692800e+06],\n",
       "        ...,\n",
       "        [4.70148352e+08, 4.70146368e+08, 4.70144576e+08, ...,\n",
       "         4.69918432e+08, 4.69916672e+08, 4.69914848e+08],\n",
       "        [4.73894656e+08, 4.73892832e+08, 4.73891008e+08, ...,\n",
       "         4.73663264e+08, 4.73661088e+08, 4.73659552e+08],\n",
       "        [4.77641152e+08, 4.77639328e+08, 4.77637344e+08, ...,\n",
       "         4.77407872e+08, 4.77405984e+08, 4.77404064e+08]],\n",
       "\n",
       "       [[4.50566240e+08, 4.50564320e+08, 4.50562464e+08, ...,\n",
       "         4.50331072e+08, 4.50329216e+08, 4.50327328e+08],\n",
       "        [4.54072992e+08, 4.54071008e+08, 4.54069216e+08, ...,\n",
       "         4.53835936e+08, 4.53833952e+08, 4.53832160e+08],\n",
       "        [4.57579488e+08, 4.57577568e+08, 4.57575616e+08, ...,\n",
       "         4.57340768e+08, 4.57338880e+08, 4.57337024e+08],\n",
       "        ...,\n",
       "        [8.88893312e+08, 8.88889280e+08, 8.88885504e+08, ...,\n",
       "         8.88429248e+08, 8.88425536e+08, 8.88421952e+08],\n",
       "        [8.92400000e+08, 8.92395840e+08, 8.92392640e+08, ...,\n",
       "         8.91934144e+08, 8.91930048e+08, 8.91926848e+08],\n",
       "        [8.95906560e+08, 8.95902784e+08, 8.95899008e+08, ...,\n",
       "         8.95438976e+08, 8.95435072e+08, 8.95431424e+08]],\n",
       "\n",
       "       [[8.37889472e+08, 8.37885632e+08, 8.37881664e+08, ...,\n",
       "         8.37420032e+08, 8.37416512e+08, 8.37412672e+08],\n",
       "        [8.41156224e+08, 8.41152256e+08, 8.41148544e+08, ...,\n",
       "         8.40684608e+08, 8.40681344e+08, 8.40676928e+08],\n",
       "        [8.44422976e+08, 8.44419264e+08, 8.44415424e+08, ...,\n",
       "         8.43949760e+08, 8.43945792e+08, 8.43942272e+08],\n",
       "        ...,\n",
       "        [1.24623334e+09, 1.24622784e+09, 1.24622234e+09, ...,\n",
       "         1.24553536e+09, 1.24552973e+09, 1.24552422e+09],\n",
       "        [1.24950042e+09, 1.24949466e+09, 1.24948915e+09, ...,\n",
       "         1.24880038e+09, 1.24879475e+09, 1.24878925e+09],\n",
       "        [1.25276710e+09, 1.25276160e+09, 1.25275546e+09, ...,\n",
       "         1.25206528e+09, 1.25205952e+09, 1.25205389e+09]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.04573926e+09, 1.04571475e+09, 1.04569024e+09, ...,\n",
       "         1.04269312e+09, 1.04266874e+09, 1.04264435e+09],\n",
       "        [1.04636710e+09, 1.04634285e+09, 1.04631866e+09, ...,\n",
       "         1.04331942e+09, 1.04329504e+09, 1.04327066e+09],\n",
       "        [1.04699565e+09, 1.04697114e+09, 1.04694694e+09, ...,\n",
       "         1.04394598e+09, 1.04392160e+09, 1.04389722e+09],\n",
       "        ...,\n",
       "        [1.12427328e+09, 1.12424678e+09, 1.12422093e+09, ...,\n",
       "         1.12099866e+09, 1.12097229e+09, 1.12094605e+09],\n",
       "        [1.12490176e+09, 1.12487578e+09, 1.12484954e+09, ...,\n",
       "         1.12162509e+09, 1.12159898e+09, 1.12157286e+09],\n",
       "        [1.12552973e+09, 1.12550362e+09, 1.12547738e+09, ...,\n",
       "         1.12225152e+09, 1.12222515e+09, 1.12219891e+09]],\n",
       "\n",
       "       [[6.96206912e+08, 6.96180736e+08, 6.96154304e+08, ...,\n",
       "         6.92926592e+08, 6.92900352e+08, 6.92874112e+08],\n",
       "        [6.96595456e+08, 6.96568640e+08, 6.96542592e+08, ...,\n",
       "         6.93313344e+08, 6.93287040e+08, 6.93260736e+08],\n",
       "        [6.96983744e+08, 6.96957312e+08, 6.96931072e+08, ...,\n",
       "         6.93699840e+08, 6.93673408e+08, 6.93647296e+08],\n",
       "        ...,\n",
       "        [7.44758400e+08, 7.44730368e+08, 7.44702080e+08, ...,\n",
       "         7.41249472e+08, 7.41221120e+08, 7.41193088e+08],\n",
       "        [7.45146816e+08, 7.45118656e+08, 7.45090432e+08, ...,\n",
       "         7.41635968e+08, 7.41607936e+08, 7.41579648e+08],\n",
       "        [7.45535232e+08, 7.45507136e+08, 7.45479040e+08, ...,\n",
       "         7.42022464e+08, 7.41994432e+08, 7.41966208e+08]],\n",
       "\n",
       "       [[2.85270048e+08, 2.85241920e+08, 2.85213760e+08, ...,\n",
       "         2.81755456e+08, 2.81727392e+08, 2.81699296e+08],\n",
       "        [2.85418592e+08, 2.85390432e+08, 2.85362272e+08, ...,\n",
       "         2.81902272e+08, 2.81874144e+08, 2.81845952e+08],\n",
       "        [2.85567008e+08, 2.85538976e+08, 2.85510816e+08, ...,\n",
       "         2.82048864e+08, 2.82020800e+08, 2.81992704e+08],\n",
       "        ...,\n",
       "        [3.03838752e+08, 3.03808800e+08, 3.03778944e+08, ...,\n",
       "         3.00095392e+08, 3.00065536e+08, 3.00035712e+08],\n",
       "        [3.03987360e+08, 3.03957280e+08, 3.03927488e+08, ...,\n",
       "         3.00242144e+08, 3.00212256e+08, 3.00182336e+08],\n",
       "        [3.04135840e+08, 3.04105984e+08, 3.04075904e+08, ...,\n",
       "         3.00388992e+08, 3.00358976e+08, 3.00329088e+08]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride = 0.03\n",
    "a = np.arange(0, 16*128*128*stride, stride, dtype=\"float32\").reshape(16, 128, 128)\n",
    "b = np.arange(16*128*128*stride, 0, -stride, dtype=\"float32\").reshape(16, 128, 128)\n",
    "c_lnumpy = np.empty((16, 128, 128), dtype=np.float32)\n",
    "lnumpy_mm_relu_v2(a, b , c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_lib = tvm.build(MyBmmRelu, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((16, 128, 128), dtype=np.float32))\n",
    "rt_lib[\"bmm_relu\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_lnumpy, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目标程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class TargetModule:\n",
    "    @T.prim_func\n",
    "    def bmm_relu(A: T.Buffer((16, 128, 128), \"float32\"), B: T.Buffer((16, 128, 128), \"float32\"), C: T.Buffer((16, 128, 128), \"float32\")) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
    "        for i0 in T.parallel(16):\n",
    "            for i1, i2_0 in T.grid(128, 16):\n",
    "                for ax0_init in T.vectorized(8):\n",
    "                    with T.block(\"Y_init\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n",
    "                        Y[n, i, j] = T.float32(0)\n",
    "                for ax1_0 in T.serial(32):\n",
    "                    for ax1_1 in T.unroll(4):\n",
    "                        for ax0 in T.serial(8):\n",
    "                            with T.block(\"Y_update\"):\n",
    "                                n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                                j = T.axis.spatial(128, i2_0 * 8 + ax0)\n",
    "                                k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n",
    "                                Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "                for i2_1 in T.vectorized(8):\n",
    "                    with T.block(\"C\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n",
    "                        C[n, i, j] = T.max(Y[n, i, j], T.float32(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">bmm_relu</span>(\n",
       "        A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;bmm_relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        Y <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">16</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> i, j_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">16</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> j_1_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_init&quot;</span>):\n",
       "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1_init)\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
       "                        Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> k_0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">32</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">for</span> k_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>unroll(<span style=\"color: #008000\">4</span>):\n",
       "                        <span style=\"color: #008000; font-weight: bold\">for</span> j_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
       "                            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_update&quot;</span>):\n",
       "                                vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                                vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
       "                                vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">128</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
       "                                Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (\n",
       "                                    Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vn, vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vn, vk, vj]\n",
       "                                )\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0)\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj])\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vn, vi, vj])\n",
       "                        C[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Y[vn, vi, vj], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "block_Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "block_C = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
    "# Step 2. Get loops\n",
    "n, i, j, k = sch.get_loops(block_Y)\n",
    "# Step 3. Organize the loops\n",
    "j_0, j_1 = sch.split(j, factors=[None, 8])\n",
    "k_0, k_1 = sch.split(k, factors=[None, 4])\n",
    "sch.reorder(k_0, k_1, j_1)\n",
    "\n",
    "# Step 5. vectorize / parallel / unroll\n",
    "sch.parallel(n)\n",
    "sch.unroll(k_1)\n",
    "sch.vectorize(j_1)\n",
    "\n",
    "sch.reverse_compute_at(block_C, j_0)\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(block_Y, k_0)\n",
    "n, i, j0, ax0 = sch.get_loops(block_C)\n",
    "sch.vectorize(ax0)\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\hjs\\tvm\\src\\node\\structural_equal.cc\", line 376\nValueError: StructuralEqual check failed, caused by lhs at <root>.functions[I.GlobalVar(\"bmm_relu\")].body.block.body.body.body.body.seq[1].body.body.<unknown attribute>:\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def bmm_relu(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": T.bool(True)})\n        A = T.match_buffer(A_handle, (16, 128, 128))\n        B = T.match_buffer(B_handle, (16, 128, 128))\n        C = T.match_buffer(C_handle, (16, 128, 128))\n        with T.block(\"root\"):\n            T.reads()\n            T.writes()\n            Y = T.alloc_buffer((16, 128, 128))\n            for n in T.parallel(16):\n                for i in range(128):\n                    for j_0 in range(16):\n                        for j_1_init in T.vectorized(8):\n                            with T.block(\"Y_init\"):\n                                vn = T.axis.spatial(16, n)\n                                vi = T.axis.spatial(128, i)\n                                vj = T.axis.spatial(128, j_0 * 8 + j_1_init)\n                                T.reads()\n                                T.writes(Y[vn, vi, vj])\n                                Y[vn, vi, vj] = T.float32(0)\n                        for k_0 in range(32):\n                            for k_1 in T.unroll(4):\n                                for j_1 in T.vectorized(8):\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                    with T.block(\"Y_update\"):\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vn = T.axis.spatial(16, n)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vi = T.axis.spatial(128, i)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vj = T.axis.spatial(128, j_0 * 8 + j_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vk = T.axis.reduce(128, k_0 * 4 + k_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.writes(Y[vn, vi, vj])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^\n                                        Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                        for ax0 in T.vectorized(8):\n                            with T.block(\"C\"):\n                                vn = T.axis.spatial(16, n)\n                                vi = T.axis.spatial(128, i)\n                                vj = T.axis.spatial(128, j_0 * 8 + ax0)\n                                T.reads(Y[vn, vi, vj])\n                                T.writes(C[vn, vi, vj])\n                                C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\nand rhs at <root>.functions[I.GlobalVar(\"bmm_relu\")].body.block.body.body.body.body.seq[1].body.body.<unknown attribute>:\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def bmm_relu(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": T.bool(True)})\n        A = T.match_buffer(A_handle, (16, 128, 128))\n        B = T.match_buffer(B_handle, (16, 128, 128))\n        C = T.match_buffer(C_handle, (16, 128, 128))\n        with T.block(\"root\"):\n            T.reads()\n            T.writes()\n            Y = T.alloc_buffer((16, 128, 128))\n            for i0 in T.parallel(16):\n                for i1 in range(128):\n                    for i2_0 in range(16):\n                        for ax0_init in T.vectorized(8):\n                            with T.block(\"Y_init\"):\n                                n = T.axis.spatial(16, i0)\n                                i = T.axis.spatial(128, i1)\n                                j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n                                T.reads()\n                                T.writes(Y[n, i, j])\n                                Y[n, i, j] = T.float32(0)\n                        for ax1_0 in range(32):\n                            for ax1_1 in T.unroll(4):\n                                for ax0 in range(8):\n                                ^^^^^^^^^^^^^^^^^^^^\n                                    with T.block(\"Y_update\"):\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        n = T.axis.spatial(16, i0)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        i = T.axis.spatial(128, i1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        j = T.axis.spatial(128, i2_0 * 8 + ax0)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.reads(Y[n, i, j], A[n, i, k], B[n, k, j])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.writes(Y[n, i, j])\n                                        ^^^^^^^^^^^^^^^^^^^^\n                                        Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                        for i2_1 in T.vectorized(8):\n                            with T.block(\"C\"):\n                                n = T.axis.spatial(16, i0)\n                                i = T.axis.spatial(128, i1)\n                                j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n                                T.reads(Y[n, i, j])\n                                T.writes(C[n, i, j])\n                                C[n, i, j] = T.max(Y[n, i, j], T.float32(0))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24844\\819329240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_structural_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\tvm-build\\lib\\site-packages\\tvm-0.14.dev37+g02ffc9139-py3.7-win-amd64.egg\\tvm\\ir\\base.py\u001b[0m in \u001b[0;36massert_structural_equal\u001b[1;34m(lhs, rhs, map_free_vars)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0mlhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[0mrhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     \u001b[0m_ffi_node_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructuralEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_free_vars\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore # pylint: disable=no-member\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\tvm-build\\lib\\site-packages\\tvm-0.14.dev37+g02ffc9139-py3.7-win-amd64.egg\\tvm\\_ffi\\_ctypes\\packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         ):\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTVMError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\hjs\\tvm\\src\\node\\structural_equal.cc\", line 376\nValueError: StructuralEqual check failed, caused by lhs at <root>.functions[I.GlobalVar(\"bmm_relu\")].body.block.body.body.body.body.seq[1].body.body.<unknown attribute>:\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def bmm_relu(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": T.bool(True)})\n        A = T.match_buffer(A_handle, (16, 128, 128))\n        B = T.match_buffer(B_handle, (16, 128, 128))\n        C = T.match_buffer(C_handle, (16, 128, 128))\n        with T.block(\"root\"):\n            T.reads()\n            T.writes()\n            Y = T.alloc_buffer((16, 128, 128))\n            for n in T.parallel(16):\n                for i in range(128):\n                    for j_0 in range(16):\n                        for j_1_init in T.vectorized(8):\n                            with T.block(\"Y_init\"):\n                                vn = T.axis.spatial(16, n)\n                                vi = T.axis.spatial(128, i)\n                                vj = T.axis.spatial(128, j_0 * 8 + j_1_init)\n                                T.reads()\n                                T.writes(Y[vn, vi, vj])\n                                Y[vn, vi, vj] = T.float32(0)\n                        for k_0 in range(32):\n                            for k_1 in T.unroll(4):\n                                for j_1 in T.vectorized(8):\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                    with T.block(\"Y_update\"):\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vn = T.axis.spatial(16, n)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vi = T.axis.spatial(128, i)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vj = T.axis.spatial(128, j_0 * 8 + j_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        vk = T.axis.reduce(128, k_0 * 4 + k_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.writes(Y[vn, vi, vj])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^\n                                        Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                        for ax0 in T.vectorized(8):\n                            with T.block(\"C\"):\n                                vn = T.axis.spatial(16, n)\n                                vi = T.axis.spatial(128, i)\n                                vj = T.axis.spatial(128, j_0 * 8 + ax0)\n                                T.reads(Y[vn, vi, vj])\n                                T.writes(C[vn, vi, vj])\n                                C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\nand rhs at <root>.functions[I.GlobalVar(\"bmm_relu\")].body.block.body.body.body.body.seq[1].body.body.<unknown attribute>:\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def bmm_relu(A_handle: T.handle, B_handle: T.handle, C_handle: T.handle):\n        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": T.bool(True)})\n        A = T.match_buffer(A_handle, (16, 128, 128))\n        B = T.match_buffer(B_handle, (16, 128, 128))\n        C = T.match_buffer(C_handle, (16, 128, 128))\n        with T.block(\"root\"):\n            T.reads()\n            T.writes()\n            Y = T.alloc_buffer((16, 128, 128))\n            for i0 in T.parallel(16):\n                for i1 in range(128):\n                    for i2_0 in range(16):\n                        for ax0_init in T.vectorized(8):\n                            with T.block(\"Y_init\"):\n                                n = T.axis.spatial(16, i0)\n                                i = T.axis.spatial(128, i1)\n                                j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n                                T.reads()\n                                T.writes(Y[n, i, j])\n                                Y[n, i, j] = T.float32(0)\n                        for ax1_0 in range(32):\n                            for ax1_1 in T.unroll(4):\n                                for ax0 in range(8):\n                                ^^^^^^^^^^^^^^^^^^^^\n                                    with T.block(\"Y_update\"):\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        n = T.axis.spatial(16, i0)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        i = T.axis.spatial(128, i1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        j = T.axis.spatial(128, i2_0 * 8 + ax0)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.reads(Y[n, i, j], A[n, i, k], B[n, k, j])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                        T.writes(Y[n, i, j])\n                                        ^^^^^^^^^^^^^^^^^^^^\n                                        Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                        for i2_1 in T.vectorized(8):\n                            with T.block(\"C\"):\n                                n = T.axis.spatial(16, i0)\n                                i = T.axis.spatial(128, i1)\n                                j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n                                T.reads(Y[n, i, j])\n                                T.writes(C[n, i, j])\n                                C[n, i, j] = T.max(Y[n, i, j], T.float32(0))"
     ]
    }
   ],
   "source": [
    "tvm.ir.assert_structural_equal(sch.mod, TargetModule)\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2\n",
    "若把 sch.parallel(n) 放在 decompose reduction 之后(按照exercice顺序)会报错\n",
    "\n",
    "答疑见：https://github.com/mlc-ai/mlc-zh/discussions/35\n",
    "\n",
    "原因是 decompose reduction 之后，三个 block 全都既不是 local complete block 也不是 local reduction block。具体因为什么条件不满足，之后有机会再仔细看吧(TODO)\n",
    "\n",
    "所以要把 sch.parallel(n) 放在 decompose reduction 之前\n",
    "\n",
    "另：\n",
    "- 本题解答：https://github.com/mlc-ai/mlc-zh/discussions/145\n",
    "- 中间变量y是否有必要：https://github.com/mlc-ai/mlc-zh/discussions/32\n",
    "- parallel/unroll/vectorize的区别：https://github.com/mlc-ai/mlc-zh/discussions/82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">bmm_relu</span>(\n",
       "        A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;bmm_relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        Y <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> n <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">16</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> i, j_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">16</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> j_1_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_init&quot;</span>):\n",
       "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1_init)\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
       "                        Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> k_0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">32</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">for</span> k_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>unroll(<span style=\"color: #008000\">4</span>):\n",
       "                        <span style=\"color: #008000; font-weight: bold\">for</span> j_1 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">8</span>):\n",
       "                            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_update&quot;</span>):\n",
       "                                vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                                vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
       "                                vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">128</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
       "                                Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (\n",
       "                                    Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vn, vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[vn, vk, vj]\n",
       "                                )\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
       "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0)\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj])\n",
       "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vn, vi, vj])\n",
       "                        C[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Y[vn, vi, vj], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "block_Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "block_C = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
    "# Step 2. Get loops\n",
    "n, i, j, k = sch.get_loops(block_Y)\n",
    "# Step 3. Organize the loops\n",
    "j_0, j_1 = sch.split(j, factors=[None, 8])\n",
    "k_0, k_1 = sch.split(k, factors=[None, 4])\n",
    "sch.reorder(k_0, k_1, j_1)\n",
    "sch.reverse_compute_at(block_C, j_0)\n",
    "\n",
    "sch.parallel(n)\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(block_Y, k_0)\n",
    "\n",
    "# Step 5. vectorize / parallel / unroll\n",
    "Y_init = sch.get_block(\"Y_init\", func_name=\"bmm_relu\")\n",
    "n, i, j0, j1 = sch.get_loops(Y_init)\n",
    "sch.vectorize(j1)\n",
    "\n",
    "sch.unroll(k_1)\n",
    "\n",
    "C = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
    "n, i, j0, j1 = sch.get_loops(C)\n",
    "sch.vectorize(j1)\n",
    "\n",
    "sch.mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "tvm.ir.assert_structural_equal(sch.mod, TargetModule)\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "  44.0825      44.0825      44.0825      44.0825       0.0000                  \n",
      "After transformation:\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   6.2044       6.2044       6.2044       6.2044       0.0000                  \n"
     ]
    }
   ],
   "source": [
    "before_rt_lib = tvm.build(MyBmmRelu, target=\"llvm\")\n",
    "after_rt_lib = tvm.build(sch.mod, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "b_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "c_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "after_rt_lib[\"bmm_relu\"](a_tvm, b_tvm, c_tvm)\n",
    "before_timer = before_rt_lib.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
    "print(\"Before transformation:\")\n",
    "print(before_timer(a_tvm, b_tvm, c_tvm))\n",
    "\n",
    "f_timer = after_rt_lib.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
    "print(\"After transformation:\")\n",
    "print(f_timer(a_tvm, b_tvm, c_tvm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-build",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
